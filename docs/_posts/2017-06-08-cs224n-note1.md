---
layout: post
title:  "cs224n Lecture Notes 1"
date:   2017-06-03 14:00:00 +0800
categories: [machine learning, nlp]
tags: []
description: This set of notes begins by introducing the concept of Natural Language Processing (NLP) and the problems NLP faces today. We then move forward to discuss the concept of representing words as numeric vectors. Lastly, we discuss popular approaches to designing word vectors.
---

> Keyphrases: 
> 
> Natural Language Processing. Word Vectors. Singular Value Decomposition. Skip-gram. Continuous Bag of Words (CBOW). Negative Sampling. Hierarchical Softmax. Word2Vec.

**description**: 
This set of notes begins by introducing the concept of Natural Language Processing (NLP) and the problems NLP faces today. We then move forward to discuss the concept of representing words as numeric vectors. Lastly, we discuss popular approaches to designing word vectors.

## Introduction to Natural Language Processing

Examples of tasks

Easy

- Spell Checking
- Keyword Search
- Finding Synonyms

Medium

- Parsing information from websites, documents, etc.

Hard

- Machine Translation (e.g. Translate Chinese text to English)

- Semantic Analysis (What is the meaning of query statement?)

- Coreference (e.g. What does "he" or "it" refer to given a document?)

- Question Answering (e.g. Answering Jeopardy questions)

## Iteration Based Methods - Word2vec

Instead of computing and storing **global information** about some huge dataset (which might be billions of sentences), we can try to create a model that will be able to **learn one iteration at a time** and eventually be able to encode the probability of a word given its context.

#### Language Models (Unigrams, Bigrams, etc.)

The probability on any given sequence of n words:

$$P(w_1, w_2,..., w_n)$$

Bigram model:

$$P(w_1, w_2,..., w_n) = \prod_{i=2}^{n}P(w_i|w_{i-1})$$

#### Continuous Bag of Words Model (CBOW)

Predicting a center word from the surrounding context.

**Notation for CBOW Model:**

- $w_i$: Word i from vocabulary V
- $V\in R^{n\times|V|}$: Input word matrix
- $v_i$: i-th column of V, the input vector representation of word $w_i$
- $U\in R^{|V|\times n}$: Output word matrix
- $u_i$: i-th row of U, the output vector representation of word $w_i$

1. We generate our one hot word vectors for the input context $(x^{c-m},...,x^{c-1},x^{c+1},...x^{c+m} \in R^{|V|})$
1. We get our embedded word vectors for the context $(v_{c-m}=Vx^{c-m},...,v_{c+m}=Vx^{c+m} \in R^{n})$
1. Average these vectors to get $\hat{v} = \frac{v_{c-m}+...+v_{c+m}}{2m}$
1. Generate a score vector $z=U\hat{v}\in R^{|V|}$
1. Turn the scores into probabilities $\hat{y}=softmax(z)\in R^{|V|}$

$$minimize J = -logP(w_c|w_{c-m},...,w_{c+m}) = -logP(u_c|\hat{v})$$

<center>
<img src="{{ site.baseurl }}/assets/pic/10_01.png" height="300px" >
</center>

#### Skip-Gram Model

Predicting surrounding context words given a center word.

1. We generate our one hot input vector $(x \in R^{|V|})$ of the center word
1. We get our embedded word vectors for the context $(v_c=Vx \in R^{n})$
1. Generate a score vector $z=Uv_c$
1. Turn the scores into probabilities $\hat{y}=softmax(z)\in R^{|V|}$.note that $(\hat{y}_{c-m},...,\hat{y}_{c-1},\hat{y}_{c+1},...\hat{y}_{c+m})$ are the probabilities of observing each context word

**Assumption**:given the center word, all output words are completely independent

$$minimize J = -logP(w_{c-m},...,w_{c+m}|w_c) = -log\prod_{j=0,j\ne m}^{2m}P(u_{c-m+j}|v_c)$$

<center>
<img src="{{ site.baseurl }}/assets/pic/10_02.png" height="300px" >
</center>

#### Negative Sampling

Note that the summation over $|V|$ is computationally huge.A simple idea is we could instead just approximate it.We "sample" from a noise distribution $P_n(w)$ whose probabilities match the ordering of the frequency of the vocabulary.


