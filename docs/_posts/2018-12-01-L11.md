---
layout: post
title:  "CS294 L11 Model-Based Reinforcement Learning"
date:   2018-12-01 10:00:00 +0800
categories: [deep Reinforcement Learning]
tags: []
description: 
---

## 基于模型的增强学习：框架

如果我们知道模型动态$f(\mathbf{s}_ t,\mathbf{a}_ t)=\mathbf{s}_ {t+1}$，或者随机的分布$p(\mathbf{s}_{t+1}\vert \mathbf{s}_t,\mathbf{a}_t)$，那么我们就可以使用上一篇的方法。因此我们考虑去从数据中学习$f(\mathbf{s}_t,\mathbf{a}_t)$，这样的方法被统称为**基于模型的增强学习（model-based reinforcement learning）**，其基本步骤为（v0.5）：
1. 运行某种基本策略$\pi_0(\mathbf{a}_t\vert \mathbf{s}_t)$（如随机策略）来收集样本数据$\mathcal{D}=\lbrace(\mathbf{s},\mathbf{a},\mathbf{s}')_i\rbrace$。
2. 通过最小化$\sum_i\Vert f(\mathbf{s}_i,\mathbf{a}_i)-\mathbf{s}_i'\Vert^2$的方法来学习模型动态$f(\mathbf{s},\mathbf{a})$。
3. 根据$f(\mathbf{s},\mathbf{a})$来计划未来行动。

这是经典机器人学**系统识别（system identification）**中使用的方法，即已经有了参数式的机器人的运动方程，需要收集数据来估计参数，收集数据的基本策略可以是随机的。v0.5版本有个严重的问题：分布不匹配（类似模仿学习）。我们通过分布$p_{\pi_0}(\mathbf{x}_ t)$来收集数据，但是实际用于规划行动时，我们遇到的是$p_{\pi_f}(\mathbf{x}_ t)$。即便我们训练了一个在分布$p_{\pi_0}(\mathbf{x}_ t)$下很好的模型，但这个模型在$p_{\pi_f}(\mathbf{x}_t)$所遇到的状态下可以很差。并且，这样的分布不匹配问题在使用越具表达力的模型簇时越严重。如果像我们跟前面所说的一样只缺少几个待定参数，那么其实对数据要求还是不高的；而使用如深度神经网络这样具有高表达力的模型，则会把红色部分的数据拟合得相当好（过拟合），然后产生更大的问题。

因此，需要收集更多“实际分布”下的数据，改进的（v1.0）版本如下：
1. 运行某种基本策略$\pi_0(\mathbf{a}_t\vert \mathbf{s}_t)$（如随机策略）来收集样本数据$\mathcal{D}=\lbrace(\mathbf{s},\mathbf{a},\mathbf{s}')_i\rbrace$。
2. 通过最小化$\sum_i\Vert f(\mathbf{s}_i,\mathbf{a}_i)-\mathbf{s}_i'\Vert^2$的方法来学习模型动态$f(\mathbf{s},\mathbf{a})$。
3. 根据$f(\mathbf{s},\mathbf{a})$来计划未来行动。
4. 执行这些行动，并得到一系列结果数据$\lbrace(\mathbf{s},\mathbf{a},\mathbf{s}')_j\rbrace$加入到$\mathcal{D}$中。反复执行2-4步。

与DAgger不同？？？？

考虑进一步的改进。错误并不只是致命错误，对于小错误，我们希望当小错误累积到一定程度时，重新规划方法。v1.5版本如下：
1. 运行某种基本策略$\pi_0(\mathbf{a}_t\vert \mathbf{s}_t)$（如随机策略）来收集样本数据$\mathcal{D}=\lbrace(\mathbf{s},\mathbf{a},\mathbf{s}')_i\rbrace$。
2. 通过最小化$\sum_i\Vert f(\mathbf{s}_i,\mathbf{a}_i)-\mathbf{s}_i'\Vert^2$的方法来学习模型动态$f(\mathbf{s},\mathbf{a})$。
3. 根据$f(\mathbf{s},\mathbf{a})$来计划未来行动。（如使用iLQR）
4. 基于MPC的思想？？？？？？，仅执行计划中的第一步行动，观察到新的状态$\mathbf{s}'$。
5. 将这一组新的$(\mathbf{s},\mathbf{a},\mathbf{s}')$加入到$\mathcal{D}$中。反复执行若干次3-5步之后，回到第2步重新学习模型。

在这个v1.5版本中，最难的一点是做规划。越精密的模型和方法，计划未来行动的代价越大。MPC方法可能很好，但是计算代价有可能会很大，因此采用构造一个策略函数$\pi_\theta(\mathbf{s}_t)$的方法来得到具体的行动，即v2.0版本：
1. 运行某种基本策略$\pi_0(\mathbf{a}_t\vert \mathbf{s}_t)$（如随机策略）来收集样本数据$\mathcal{D}=\lbrace(\mathbf{s},\mathbf{a},\mathbf{s}')_i\rbrace$。
2. 通过最小化$\sum_i\Vert f(\mathbf{s}_i,\mathbf{a}_i)-\mathbf{s}_i'\Vert^2$的方法来学习模型动态$f(\mathbf{s},\mathbf{a})$。
3. 通过$f(\mathbf{s},\mathbf{a})$，使用反向传播的方法来优化策略函数$\pi_\theta(\mathbf{a}_t\vert \mathbf{s}_t)$。
4. 执行$\pi_\theta(\mathbf{a}_t\vert \mathbf{s}_t)$，将新的$(\mathbf{s},\mathbf{a},\mathbf{s}')$加入到$\mathcal{D}$中。反复执行2-4步。

<center>
<img src="{{ site.baseurl }}/assets/pic/L11_0.jpg" height="150px" >
</center>

需要明确的是，$f$只是去最小化损失函数，而$\pi$则是通过有点类似于BPTT训练RNN的方法进行求解的。当然这个方法本身并不是很好的，主要原因和训练RNN难度很大一致。

## 从全局模型到局部模型
之前讨论的模型是**全局模型**，即学习的$f(\mathbf{s}_t,\mathbf{a}_t)$对整个系统都起作用。出于这个目的，我们使用神经网络去拟合。它的问题在于：如果一开始估计错误，它会去探索因模型错误而被乐观估计的区域，这是因为大多数状态模型能很好地估计，因为总有一些状态模型会出错误。某些情况下，模型比行动策略要复杂，比如拿杯子，策略很简单，模型却要考虑各种物理关系等等。因此，我们想训练一个**局部模型**，通过梯度等局部信息来表征？？？？

<center>
<img src="{{ site.baseurl }}/assets/pic/L11_1.jpg" height="300px" >
</center>

我们需要梯度信息$\frac{\mathrm{d}f}{\mathrm{d}\mathbf{x}_ t},\frac{\mathrm{d}f}{\mathrm{d}\mathbf{u}_ t}$。假设模型动态是一个高斯分布$p(\mathbf{x}_{t+1}\vert \mathbf{x}_t,\mathbf{u}_t)=\mathcal{N}(f(\mathbf{x}_t,\mathbf{u}_t),\Sigma)$，其中$f(\mathbf{x}_t,\mathbf{u}_t)\approx \mathbf{A}_t\mathbf{x}_t+\mathbf{B}_t\mathbf{u}_t$。这样的假设保证给定时间内均值是线性的，并且只需拟合$(\mathbf{A}_t,\mathbf{B}_t)$，就能得到微分信息，并使用iLQR方法改进策略函数$\pi(\mathbf{u}_t\vert \mathbf{x}_t)$。

第一个问题，使用怎样的策略收集数据？由于目前使用局部模型，因此随机选择策略很难拟合当前策略的局部信息。