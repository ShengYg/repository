---
layout: post
title:  "CS294 L16 Inverse Reinforcement Learning"
date:   2018-12-12 10:00:00 +0800
categories: [deep Reinforcement Learning]
tags: []
description: 1、有些情况下的收益函数很难定义，因此使用逆增强学习学习收益函数；2、特征匹配逆增强学习像SVM一样最大化间隔，但难以优化；2、最大熵逆IRL解决了人类示范不是最优这种情况，基于表格法，适合有限状态空间；3、深度IRL可以用于连续空间，可以不假设有模型存在，较广泛；4、深度IRL和GAN的思路很相似。
---

- 目录
{:toc #markdown-toc}

## 定义与存在问题
### 定义
- 增强学习：状态空间$\mathbf{s}\in\mathcal{S}$，决策空间$\mathbf{a}\in\mathcal{A}$，系统转移动态$p(\mathbf{s}'\vert\mathbf{s},\mathbf{a})$，收益函数$r(\mathbf{s},\mathbf{a})$ $\Rightarrow$ 最优策略$\pi^*(\mathbf{a}\vert\mathbf{s})$
- 逆增强学习：状态空间$\mathbf{s}\in\mathcal{S}$，决策空间$\mathbf{a}\in\mathcal{A}$，系统转移动态$p(\mathbf{s}'\vert\mathbf{s},\mathbf{a})$，从最优策略下的轨迹分布$\pi^* (\tau)$中抽取出来的轨迹样本$\{\tau_i\}$ $\Rightarrow$ 收益函数$r_\psi(\mathbf{s},\mathbf{a})$，其中$\psi$是参数，再使用它来学习最优策略$\pi^*(\mathbf{a}\vert\mathbf{s})$

### 存在问题
在逆增强学习问题中，我们要尝试去改进收益函数，然而去评估收益函数的时候，我们要求解类似梯度的东西，这其实是正增强学习问题要做的事情，因此有点类似于正增强学习是逆增强学习内循环中的子问题，在样本使用和计算上都很困难。

## 早期逆增强学习算法
### 特征匹配逆增强学习（Feature matching IRL）
假定收益函数是若干特征函数的线性组合$r_\psi(\mathbf{s},\mathbf{a})=\psi^\top\mathbf{f}(\mathbf{s},\mathbf{a})$，如果这些特征选得比较好，我们只需要选权重的话，我们就会考虑去匹配最优策略下的期望。即对于一个收益函数$r_\psi$，最优策略为$\pi^{r_\psi}$，那么我们就希望去找到一组参数$\psi$，使得$\mathbf{E}_ {\pi^{r_\psi}}[\mathbf{f}(\mathbf{s},\mathbf{a})]=\mathbf{E}_ {\pi^* }[\mathbf{f}(\mathbf{s},\mathbf{a})]$，左边是边缘化的期望，右边是未知的最优专家策略。我们可以从SVM中借鉴最大间隔（maximum margin）原理来得到一个比较靠谱的解，如

$$\max_{\psi,m}m \qquad \text{ s.t. }\psi^\top\mathbf{E}_ {\pi^*}[\mathbf{f}(\mathbf{s},\mathbf{a})]\geq\psi^\top\max_{\pi\in\Pi}\mathbf{E}_\pi[\mathbf{f}(\mathbf{s},\mathbf{a})]+m$$

也就是找一个分割超平面把**最优解下的期望收益**和策略簇$\Pi$内**其他所有策略的期望收益**相区分开，并且使得间隔m最大。不难发现，如果$\pi^* \in\Pi$，那么最优解落在分割超平面上，这个间隔就不起效果了。因此，可能这样一刀切的间隔是不好的，我们有必要去体现策略不同下期望收益和专家策略有差异（专家策略就应该间距为0），使得和专家策略相差得越多，策略越糟糕。定义两个策略的距离为$D(\pi,\pi^* )$，那么使用一个与SVM类似的技巧，

$$\min_\psi\frac{1}{2}\Vert\psi\Vert^2 \qquad \text{ s.t. }\psi^\top\mathbf{E}_ {\pi^* }[\mathbf{f}(\mathbf{s},\mathbf{a})]\geq\max_{\pi\in\Pi}\psi^\top\mathbf{E}_\pi[\mathbf{f}(\mathbf{s},\mathbf{a})]+D(\pi,\pi^*)$$

改进后的方法还是有很多问题。1. **很难说明为什么间距大就代表收益大**。当专家的策略不是最优时，这个最优化问题根本就无解。2. 这个带约束的优化问题**很复杂**，很难推广到复杂的非线性神经网络。

### 最大熵逆增强学习
和上一章一样的公式，将参数化的收益函数的参数$\psi$引入，得到基本假设$p(\mathcal{O}_ t|\mathbf{s}_ t,\mathbf{a}_ t,\psi)\propto\exp(r_\psi(\mathbf{s}_ t,\mathbf{a}_ t))$以及轨迹条件概率$p(\tau|\mathcal{O}_ {1:T},\psi)\propto p(\tau)\exp\left(\sum_tr_\psi(\mathbf{s}_ t,\mathbf{a}_ t)\right)$。我们的目标是学习收益参数，且最优变量取决于$\psi$。现有的数据是从最优策略下的轨迹分布$\pi^*(\tau)$中抽样得到的轨迹样本$\{\tau_i\}$。学习的方法是最大化对数似然函数$\max_\psi\frac{1}{N}\sum_{i=1}^N\log p(\tau_i|\mathcal{O}_ {1:T},\psi)$，忽略常数$p(\tau)$，得到结果$\max_\psi\frac{1}{N}\sum_{i=1}^Nr_\psi(\tau_i)-\log Z$，其中$Z=\int p(\tau)\exp(r_\psi(\tau))\mathrm{d}\tau$为归一化项。

对对数似然函数关于参数求梯度有：

$$
\begin{align}
\nabla_\psi\mathcal{L}
&= \frac{1}{N}\sum_{i=1}^N\nabla_\psi r_\psi(\tau_i)-\int \frac{1}{Z}p(\tau)\exp(r_\psi(\tau))\nabla_\psi r_\psi(\tau)\mathrm{d}\tau \\
&=\mathbf{E}_ {\tau\sim\pi^*(\tau)}[\nabla_\psi r_\psi(\tau)]-\mathbf{E}_ {\tau\sim p(\tau|\mathcal{O}_ {1:T},\psi)}[\nabla_\psi r_\psi(\tau)]
\end{align}
$$

前者为**专家策略下的轨迹分布下的收益关于参数的梯度**，与数据相关，可以从专家样本中估计。后者为**当前收益函数对应的软化最优策略下的轨迹分布下的收益关于参数的梯度**，是从当前策略进行采样。把第二块期望的轨迹收益按照时间拆开：

$$
\begin{align}
\mathbf{E}_ {\tau\sim p(\tau|\mathcal{O}_ {1:T},\psi)}[\nabla_\psi r_\psi(\tau)]
&= \mathbf{E}_ {\tau\sim p(\tau|\mathcal{O}_ {1:T},\psi)}\left[\nabla_\psi\sum_{t=1}^Tr_\psi(\mathbf{s}_t,\mathbf{a}_t)\right] \\
&= \sum_{t=1}^T \mathbf{E}_ {(\mathbf{s}_ t,\mathbf{a}_ t)\sim p(\mathbf{s}_ t,\mathbf{a}_ t|\mathcal{O}_ {1:T},\psi)}[\nabla_\psi r_\psi(\mathbf{s}_t,\mathbf{a}_t)]
\end{align}
$$

其中$p(\mathbf{s}_ t,\mathbf{a}_ t|\mathcal{O}_ {1:T},\psi)\propto\beta(\mathbf{s}_ t,\mathbf{a}_ t)\alpha(\mathbf{s}_ t)$。令$\mu_t(\mathbf{s}_ t,\mathbf{a}_ t)\propto\beta_t(\mathbf{s}_ t,\mathbf{a}_ t)\alpha_t(\mathbf{s}_ t)$作为在t时刻状态行动$(\mathbf{s}_ t,\mathbf{a}_ t)$访问概率，那么第二块的期望就可以写成一个二重积分$\sum_{t=1}^T\int\int \mu_t(\mathbf{s}_ t,\mathbf{a}_ t)\nabla_\psi r_\psi(\mathbf{s}_ t,\mathbf{a}_ t)\mathrm{d}\mathbf{s}_ t\mathrm{d}\mathbf{a}_ t$，也可以简写为一个内积关系$\sum_{t=1}^T \vec{\mu}_ t^\top\nabla_\psi\vec{r}_\psi$。从而，我们得到了一个**最大熵逆增强学习（MaxEnt IRL）**算法：
1. 给定$\psi$，按照上一篇的方法求出对应的$\beta(\mathbf{s}_t,\mathbf{a}_t)$和$\alpha(\mathbf{s}_t)$。
2. 计算访问概率$\mu_t(\mathbf{s}_t,\mathbf{a}_t)\propto\beta_t(\mathbf{s}_t,\mathbf{a}_t)\alpha_t(\mathbf{s}_t)$。
3. 求解梯度$\nabla_\psi\mathcal{L}=\frac{1}{N}\sum_{i=1}^N\sum_{t=1}^T\nabla_\psi r_\psi(\mathbf{s}_ {i,t},\mathbf{a}_ {i,t})-\sum_{t=1}^T\int\int \mu_t(\mathbf{s}_ t,\mathbf{a}_ t)\nabla_\psi r_\psi(\mathbf{s}_t,\mathbf{a}_t)\mathrm{d}\mathbf{s}_t\mathrm{d}\mathbf{a}_t$。
4. 走一个梯度步$\psi\leftarrow\psi+\eta\nabla_\psi\mathcal{L}$。

这个算法由Ziebart et al. (2008) 发表在AAAI上“[Maximum Entropy Inverse Reinforcement Learning](http://www.aaai.org/Papers/AAAI/2008/AAAI08-227.pdf)”。之所以称为最大熵，因为原理其实与上一篇中的熵正则化类似。我们可以说明如果我们的$r_\psi(\mathbf{s}_ t,\mathbf{a}_ t)=\psi^\top\mathbf{f}(\mathbf{s}_ t,\mathbf{a}_ t)$是这样的线性形式，那么其实这样的算法是在最优化$\max_\psi\mathcal{H}(\pi^{r_\psi})\text{ s.t. }\mathbf{E}_ {\pi^{r_\psi}}[\mathbf{f}]=\mathbf{E}_{\pi^*}[\mathbf{f}]$，在保证学习的策略的特征和专家策略特征一致的基础上使得策略的熵最大。

## 深度逆增强学习
之前的算法基于表格实现，求解梯度需要枚举所有状态-行动对，然后递推求解几个动态规划问题。对于深度逆增强学习，我们希望能处理**离散较大的甚至是连续的状态行动空间**，并对**系统未知动态**做有效的学习。假设**系统状态未知**，但可以像普通增强学习一样进行**抽样**。

对于问题：

$$\nabla_\psi\mathcal{L}=\mathbf{E}_ {\tau\sim\pi^*(\tau)}[\nabla_\psi r_\psi(\tau)]-\mathbf{E}_ {\tau\sim p(\tau|\mathcal{O}_ {1:T},\psi)}[\nabla_\psi r_\psi(\tau)]$$

前者是专家样本数据中得到的，后者是当前收益函数对应最优策略下的。

方法一：使用任何最大熵增强学习方法学习出策略$p(\mathbf{a}_ t\vert\mathbf{s}_ t,\mathcal{O}_{1:T},\psi)$，然后根据这个策略来采集$\{\tau_j\}$，用专家样本的结果减掉新抽样本的结果来做无偏估计，如下

$$\nabla_\psi\mathcal{L}\approx\frac{1}{N}\sum_{i=1}^N\nabla_\psi r_\psi(\tau_i)-\frac{1}{M}\sum_{j=1}^M\nabla_\psi r_\psi(\tau_j)$$

因为我们不假定有模型，所以可能要用无模型的增强学习算法，这将使得每一步都花掉很多很多时间，代价很高。

方法二：每次把策略改进一点点，然后用这个不准确的策略去近似估计梯度。由于策略是不正确的，因此估计量有偏差。一个很有效的办法是**重要性抽样**。新的对数似然函数的梯度公式为：

$$
\nabla_\psi\mathcal{L}\approx\frac{1}{N}\sum_{i=1}^N\nabla_\psi r_\psi(\tau_i)-\frac{1}{\sum_jw_j}\sum_{j=1}^Mw_j\nabla_\psi r_\psi(\tau_j) \\
w_j=\frac{p(\tau_j)\exp(r_\psi(\tau_j))}{\pi(\tau_j)}
$$

将权重展开可以得到$w_j=\frac{p(\mathbf{s}_ 1)\prod_ tp(\mathbf{s}_ {t+1}\vert\mathbf{s}_ t,\mathbf{a}_ t)\exp(r_\psi(\mathbf{s}_ t,\mathbf{a}_ t))}{p(\mathbf{s}_ 1)\prod_tp(\mathbf{s}_ {t+1}\vert\mathbf{s}_ t,\mathbf{a}_ t)\pi(\mathbf{a}_ t\vert\mathbf{s}_ t)}=\frac{\exp(\sum_tr_\psi(\mathbf{s}_ t,\mathbf{a}_ t))}{\prod_ t\pi(\mathbf{a}_ t\vert\mathbf{s}_ t)}$。经过多次迭代达到最优时，$\pi(\tau)\propto p(\tau)\exp(r_\psi(\tau))$，此时就无需再做重要性采样。

Finn et al. (2016) 发表在ICML上的文章“[Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization](http://proceedings.mlr.press/v48/finn16.html)”提出了**引导代价学习（guided cost learning）**。给定一些人工的示范，然后算法从一个随机的策略$\pi$开始，通过运行策略$\pi$去生成随机样本，然后使用重要性抽样和人工示范的方式来更新收益函数，根据收益函数稍微更新一下分布，然后下一阶段的分布更好。最后，我们得到了最终的类似于专家的收益函数和对应的策略。

<center>
<img src="{{ site.baseurl }}/assets/pic/L16_0.jpg" height="300px" >
</center>

## 与GAN的联系
Finn et al. (2016) 发表于NIPS的文章“[A Connection between Generative Adversarial Networks, Inverse Reinforcement Learning, and Energy-Based Models](https://arxiv.org/abs/1611.03852)”进行一个对比，IRL中的轨迹对应着GAN中的样本，IRL中的策略对应着GAN中的生成器（生成轨迹和样本），而IRL收益函数则对应了GAN中的判别器。


