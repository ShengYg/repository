---
layout: post
title:  "CS294 L02 Supervised Learning of Behaviors"
date:   2018-11-21 14:00:00 +0800
categories: [deep Reinforcement Learning]
tags: []
description: 序列决策问题 模仿学习 DAgger算法
---

- 目录
{:toc #markdown-toc}

## sequential decision problems
<center>
<img src="{{ site.baseurl }}/assets/pic/L02_0.png" height="200px" >
</center>
> 状态序列$s_t$具有马尔可夫性，而观测序列$o_t$没有。

## imitation learning
### 定义
记录$(s_t, o_t)$序列，利用监督学习得到$\pi_{\theta}(a_t|o_t)$，例如无人驾驶，记录图片-方向盘。
### 缺点
在序列学习中，随着时间增加，轨迹误差会累加
<center>
<img src="{{ site.baseurl }}/assets/pic/L02_1.jpg" height="200px" >
</center>

### 解决
多采数据，限制轨迹偏离，例如无人驾驶中多个摄像头
<center>
<img src="{{ site.baseurl }}/assets/pic/L02_2.jpg" height="200px" >
</center>

### 引申：DAgger算法
对于轨迹分布$p(\tau)$，其中轨迹$\tau:=(s_1,a_1,...s_T,a_T)$，训练数据中轨迹符合特定分布$p_{data}(o_t)$，实际轨迹为$p_{\pi_{\theta}}(o_t)$，需要通过域转移(domain shift)来使得二者相等。

用$p_{\pi_{\theta}}(o_t)$去接近$p_{data}(o_t)$很难，于是反过来用$p_{data}(o_t)$去接近$p_{\pi_{\theta}}(o_t)$，这是DAgger算法的基本思路：
- 从人工数据集$D$训练出策略$p_{\pi_{\theta}}(o_t)$
- 运行策略$p_{\pi_{\theta}}(o_t)$获得新数据集$D_{\pi}$（电脑控制汽车行驶，并收集图片）
- 人工对数据集进行标注（对图片标注应该发生的行为）
- 合并数据集

算法可以理解为“向老师请教怎么学习”，而不是单纯的填鸭式教育。DAgger算法存在的问题是第三步**人工标注需要耗费时间**，但这是必不可少的，除非算法本身模仿得很精确。通常我们不能很好地学习专家行为，有两个理由：
1. 模型的前提是马尔可夫性，但专家行为不一定满足。通常会将多帧数据以RNN形式进行处理。
2. 多峰（Multimodal）效应。在无人驾驶中如果碰到障碍物，会选择向左或向右。如果采用单峰的高斯分布，结果会被“平均”，显然不合理；如果采用离散概率分布，可能会离散得过细。这类问题的解决办法有：
    1. 混合高斯分布。简单，面对高维数据会失效
    2. 隐性密度模型（Implicit Density Model）。很难训练
    3. 自回归离散化（Autoregressive Discretization）。对某些维度的连续值进行离散

### 其他应用领域
- 带有结构预测的问题：如问答系统
- 逆增强学习

