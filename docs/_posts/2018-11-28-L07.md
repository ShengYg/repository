---
layout: post
title:  "CS294 L07 Value Functions and Q-Learning"
date:   2018-11-28 14:00:00 +0800
categories: [deep Reinforcement Learning]
tags: []
description: 策略迭代 值函数迭代 Q-learning 一些值函数理论
---

- 目录
{:toc #markdown-toc}

## 策略迭代法
对于优势函数$A^\pi(\mathbf{s}_ t,\mathbf{a}_ t)=Q^\pi(\mathbf{s}_ t,\mathbf{a}_ t)-V^\pi(\mathbf{s}_ t)$，如果我们在状态$\mathbf{s}_ t$下执行$\arg\max_{\mathbf{a}_t}A^\pi(\mathbf{s}_t,\mathbf{a}_t)$得到一个当前最好的决策，这个决策必然是期望最优的，并且和策略$\pi$无关。因此，我们有了新的策略改进的思路（即蓝色方框）：

$$\pi'(\mathbf{a}_ t\vert \mathbf{s}_ t)=I\left(\mathbf{a}_ t=\arg\max_{\mathbf{a}_t}A^\pi(\mathbf{s}_t,\mathbf{a}_t)\right)$$

我们引入非常经典的**策略迭代法（Policy Iteration, PI）**。假设已知转移概率，它包含两个步骤：
1. 策略评估（Policy Evaluation）：计算所有的$A^\pi(\mathbf{s},\mathbf{a})$；
2. 策略改进（Policy Improvement）：令$\pi\leftarrow\pi'$，其中$\pi'(\mathbf{a}_ t\vert\mathbf{s}_ t)=I\left(\mathbf{a}_ t=\arg\max_{\mathbf{a}_t}A^\pi(\mathbf{s}_t,\mathbf{a}_t)\right)$。

这个过程的主要问题在于计算$A^\pi(\mathbf{s},\mathbf{a})$，而$A^\pi(\mathbf{s},\mathbf{a})=r(\mathbf{s},\mathbf{a})+\gamma\mathbf{E}[V^\pi(\mathbf{s}')]-V^\pi(\mathbf{s})$，因此只需要研究值函数。

$$
\begin{align}
V^\pi(\mathbf{s})
&\leftarrow \mathbf{E}_ {\mathbf{a}\sim\pi(\mathbf{a}|\mathbf{s})}[r(\mathbf{s},\mathbf{a})+\gamma\mathbf{E}_{\mathbf{s}'\sim p(\mathbf{s}'|\mathbf{s},\mathbf{a})}[V^\pi(\mathbf{s}')]] \\
&\leftarrow r(\mathbf{s},\pi(\mathbf{s}))+\gamma\mathbf{E}_{\mathbf{s}'\sim p(\mathbf{s}'|\mathbf{s},\pi(\mathbf{s}))}[V^\pi(\mathbf{s}')]
\end{align}
$$

第一行为Bellman方程，可以理解成已知转移概率时的全概率展开。根据算法步骤2，可以得到确定性策略，于是进一步简化为第二行。

## 值函数迭代法
由A与Q的关系可知$\arg\max_{\mathbf{a}_ t}A^\pi(\mathbf{s}_ t,\mathbf{a}_ t)=\arg\max_{\mathbf{a}_t}Q^\pi(\mathbf{s}_t,\mathbf{a}_t)$，因此可以用Q去替代算法中的A，跳过策略（原来需要由$\pi$计算V，所有a累加求期望，然后最大化得到新的$\pi$；现在由V计算Q，然后最大化得到新的V），得到更简单的**值函数迭代法（Value Iteration, VI）**：
1. 更新Q函数：$Q(\mathbf{s},\mathbf{a})=r(\mathbf{s},\mathbf{a})+\gamma\mathbf{E}[V(\mathbf{s}')]$；
2. 更新V函数：$V(\mathbf{s})=\max_\mathbf{a}Q(\mathbf{s},\mathbf{a})$。

实际使用中，状态个数十分巨大，意味着这种类似于动态规划的算法需要很大的表。因此可以用神经网络代替这个表，即输入状态，输出对应的值函数，损失函数为$\mathcal{L}(\phi)=\frac{1}{2}\left\Vert V_\phi(\mathbf{s})-\max_\mathbf{a}Q^\pi(\mathbf{s},\mathbf{a})\right\Vert^2$，这就是**拟合值函数迭代法（Fitted Value Iteration）**：
1. $\mathbf{y}_ i\leftarrow\max_{\mathbf{a}_ i}(r(\mathbf{s}_ i,\mathbf{a}_ i)+\gamma\mathbf{E}[V_\phi(\mathbf{s}_i')])$
2. $\phi\leftarrow\arg\min_\phi\frac{1}{2}\sum_i\left\Vert V_\phi(\mathbf{s}_i)-\mathbf{y}_i\right\Vert^2$

这里面最大的问题是max操作，因为实际中不可能将状态回撤来查看其它选项，尤其是当状态数量巨大的时候。如何避免？已知$Q^\pi(\mathbf{s},\mathbf{a})\leftarrow r(\mathbf{s},\mathbf{a})+\gamma\mathbf{E}_{\mathbf{s}'\sim p(\mathbf{s}'\vert \mathbf{s},\mathbf{a})}[Q^\pi(\mathbf{s}',\pi(\mathbf{s}'))]$，意味着有了Q-函数，即使没有转移概率也能运算（不需要max操作，因为当前的Q暗示已经进行过max操作了）。这称为**拟合Q函数迭代算法（Fitted Q-Iteration）**：
1. 执行某个策略，收集容量为N的数据集$\{(\mathbf{s}_i,\mathbf{a}_i,r_i,\mathbf{s}'_i)\}$；
2. $\mathbf{y}_ i\leftarrow r(\mathbf{s}_ i,\mathbf{a}_ i)+\gamma\max_{\mathbf{a}_ i'}Q_\phi(\mathbf{s}_ i',\mathbf{a}_ i')$，使用$\max_{\mathbf{a}_ i'}Q_\phi(\mathbf{s}_ i',\mathbf{a}_ i')$来近似$\mathbf{E}[V_\phi(\mathbf{s}_i')]$；
3. $\phi\leftarrow\arg\min_\phi\frac{1}{2}\sum_i\left\Vert Q_\phi(\mathbf{s}_i,\mathbf{a}_i)-\mathbf{y}_i\right\Vert^2$。反复执行2-3多次，在回到1执行。

对该算法有几点备注：
- 第一步对策略没有要求
- off-policy：$\max_{\mathbf{a}_ i'}Q_\phi(\mathbf{s}_i',\mathbf{a}_i')$只是对决策空间的一个枚举，而$r(\mathbf{s}_i,\mathbf{a}_i)$也与策略没关系。
- 只有值函数网络，没有策略梯度，没有高方差问题（算法收集了一大堆数据，而非一条轨迹，从这角度也可以解释）
- 缺点：对非线性函数近似，不确保能收敛


**在线Q迭代算法（Online Q-iteration）**：
1. 执行某个行动$\mathbf{a}_i$，收集观察数据$(\mathbf{s}_i,\mathbf{a}_i,r_i,\mathbf{s}'_i)$
2. $\mathbf{y}_ i\leftarrow r(\mathbf{s}_ i,\mathbf{a}_ i)+\gamma\max_{\mathbf{a}_ i'}Q_\phi(\mathbf{s}_i',\mathbf{a}_i')$
3. $\phi\leftarrow\phi-\alpha\frac{\mathrm{d} Q_\phi(\mathbf{s}_ i,\mathbf{a}_ i)}{\mathrm{d}\phi}(Q_\phi(\mathbf{s}_i,\mathbf{a}_i)-\mathbf{y}_i)$

该算法每轮执行一个行动，得到一个转移观察，算出目标值，做一步的随机梯度下降。第一步行动的选择最好使用$\epsilon$-贪心或Boltzmann探索，这在以后会讲。

## 值函数理论（一些大致证明，并不精确）
对于算法$V(\mathbf{s})\leftarrow\max_\mathbf{a}[r(\mathbf{s},\mathbf{a})+\gamma\mathbf{E}[V(\mathbf{s}')]]$，我们想知道它是否收敛。定义一个算子$\mathcal{B}$，使得$\mathcal{B}V=\max_\mathbf{a}r_\mathbf{a}+\gamma\mathcal{T}_ \mathbf{a}V$，$\mathcal{T}_ \mathbf{a}$指的是选择行动$\mathbf{a}$的转移矩阵。可以证明$V^* $是$\mathcal{B}$算子的一个不动点，因为$V^* (\mathbf{s})=\max_\mathbf{a}[r(\mathbf{s},\mathbf{a})+\gamma\mathbf{E}[V^* (\mathbf{s}')]]=\mathcal{B}V^*$。这个不动点代表最优策略或最优值函数。

那么是否可以收敛到这个最优点？我们有$\Vert\mathcal{B}V-\mathcal{B}\bar{V}\Vert_\infty\leq\gamma\Vert V-\bar{V}\Vert_\infty$。令$\bar{V}=V^* $，可以有$\Vert\mathcal{B}V-V^* \Vert_\infty\leq\gamma\Vert V-V^* \Vert_\infty$，即操作符每作用一次值函数，离最佳值函数就会更进一步。拟合值函数迭代算法中，有限的神经网络只能表示值函数的一个子集，如图中的蓝色直线。算法中我们从一个函数出发，进行一次$\mathcal{B}$操作，使用最小二乘回归来投影到这个函数集合。

<center>
<img src="{{ site.baseurl }}/assets/pic/L07_0.jpg" height="100px" >
</center>

我们定义投影算子$\Pi V=\arg\min_{V'\in\Omega}\frac{1}{2}\sum\Vert V'(\mathbf{s})-V(\mathbf{s})\Vert^2$，则算法可以写成$V\leftarrow\Pi\mathcal{B}V$这样的复合映射。注意$\mathcal{B}$是一个无穷模下的收缩映射，$\Pi$是一个2模下的收缩映射，但问题在于如果我们将这两个映射进行复合，那么$\Pi\mathcal{B}$并不是一个收缩映射，因为它们不是在同一个模下收缩的。

<center>
<img src="{{ site.baseurl }}/assets/pic/L07_1.jpg" height="100px" >
</center>

不用无穷模的原因是当状态空间很大的情况下，无穷模下的投影问题极其难解,只能寻求找到最优解的一个投影，但是事与愿违的是。因此我们的结论是，值函数迭代法在表格中是收敛的，但是拟合值函数迭代法在一般意义下是不收敛的，在实践中也通常不收敛，但是有一些方法来缓解这些问题。
