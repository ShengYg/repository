<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Complete Guide to Parameter Tuning in Xgboost</title>
  <meta name="description" content="Table of Contents">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://shengyg.github.io/repository/docs/jekyll/update/2017/06/19/Complete-Guide-to-Parameter-Tuning-xgboost.html">
  <link rel="alternate" type="application/rss+xml" title="ShengYg's Blog" href="https://shengyg.github.io/repository/docs/feed.xml">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">ShengYg's Blog</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
        
          
        
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Complete Guide to Parameter Tuning in Xgboost</h1>
    <p class="post-meta"><time datetime="2017-06-19T12:00:00+08:00" itemprop="datePublished">Jun 19, 2017</time></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <h2 id="table-of-contents">Table of Contents</h2>

<ol>
  <li>The XGBoost Advantage</li>
  <li>Understanding XGBoost Parameters</li>
  <li>Tuning Parameters</li>
</ol>

<hr />

<h2 id="the-xgboost-advantage">1. The XGBoost Advantage</h2>

<ol>
  <li>Regularization:
    <ul>
      <li>Standard GBM implementation has <strong>no regularization</strong> like XGBoost, therefore it also helps to reduce overfitting.</li>
      <li>In fact, XGBoost is also known as <strong>‘regularized boosting’</strong> technique.</li>
    </ul>
  </li>
  <li>Parallel Processing:
    <ul>
      <li>XGBoost implements parallel processing and is blazingly faster as compared to GBM.</li>
      <li>Check <a href="http://zhanpengfang.github.io/418home.html">this link</a> out to explore further.</li>
      <li>XGBoost also supports implementation on Hadoop.</li>
    </ul>
  </li>
  <li>High Flexibility
    <ul>
      <li>XGBoost allow users to define <strong>custom optimization objectives and evaluation criteria</strong>.</li>
      <li>This adds a whole new dimension to the model and there is no limit to what we can do.</li>
    </ul>
  </li>
  <li>Handling Missing Values
    <ul>
      <li>XGBoost has an in-built routine to handle missing values.</li>
      <li>User is required to supply a different value than other observations and pass that as a parameter. XGBoost tries different things as it encounters a missing value on each node and learns which path to take for missing values in future.</li>
    </ul>
  </li>
  <li>Tree Pruning:
    <ul>
      <li>A GBM would stop splitting a node when it encounters a negative loss in the split. Thus it is more of a <strong>greedy algorithm</strong>.</li>
      <li>XGBoost on the other hand make <strong>splits upto the max_depth</strong> specified and then start <strong>pruning</strong> the tree backwards and remove splits beyond which there is no positive gain.</li>
      <li>Another advantage is that sometimes a split of negative loss say -2 may be followed by a split of positive loss +10. GBM would stop as it encounters -2. But XGBoost will go deeper and it will see a combined effect of +8 of the split and keep both.</li>
    </ul>
  </li>
  <li>Built-in Cross-Validation
    <ul>
      <li>XGBoost allows user to run a cross-validation at each iteration of the boosting process and thus it is easy to get the exact optimum number of boosting iterations in a single run.</li>
      <li>This is unlike GBM where we have to run a grid-search and only a limited values can be tested.</li>
    </ul>
  </li>
  <li>Continue on Existing Model
    <ul>
      <li>User can start training an XGBoost model from its last iteration of previous run. This can be of significant advantage in certain specific applications.</li>
      <li>GBM implementation of sklearn also has this feature so they are even on this point.</li>
    </ul>
  </li>
</ol>

<h2 id="xgboost-parameters">2. XGBoost Parameters</h2>

<ol>
  <li><strong>General Parameters</strong>: Guide the overall functioning</li>
  <li><strong>Booster Parameters</strong>: Guide the individual booster (tree/regression) at each step</li>
  <li><strong>Learning Task Parameters</strong>: Guide the optimization performed</li>
</ol>

<h3 id="general-parameters">2.1 General Parameters</h3>

<ol>
  <li>booster [default=gbtree]</li>
  <li>silent [default=0]</li>
  <li>nthread [default to maximum number of threads available if not set]</li>
</ol>

<h3 id="booster-parameters">2.2 Booster Parameters</h3>

<ol>
  <li>eta [default=0.3]
    <ul>
      <li>Typical final values to be used: 0.01-0.2</li>
    </ul>
  </li>
  <li>min_child_weight [default=1]
    <ul>
      <li>Defines the minimum sum of weights of all observations required in a child.</li>
      <li>This is similar to <strong>min_child_leaf</strong> in GBM but not exactly. This refers to min “sum of weights” of observations while GBM has min “number of observations”.</li>
      <li>Used to control <strong>over-fitting</strong>. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.</li>
      <li>Too high values can lead to under-fitting hence, it should be tuned using CV.</li>
    </ul>
  </li>
  <li>max_depth [default=6]
    <ul>
      <li>Used to control <strong>over-fitting</strong> as higher depth will allow model to learn relations very specific to a particular sample.</li>
      <li>Typical values: 3-10</li>
    </ul>
  </li>
  <li>max_leaf_nodes
    <ul>
      <li>The maximum number of terminal nodes or leaves in a tree.</li>
      <li>Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves.</li>
      <li>If this is defined, GBM will ignore max_depth.</li>
    </ul>
  </li>
  <li>gamma [default=0]
    <ul>
      <li>A node is split only when the resulting split gives a positive reduction in the loss function. Gamma specifies the minimum loss reduction required to make a split.</li>
      <li>Makes the algorithm conservative. The values can vary <strong>depending on the loss function</strong> and should be tuned.</li>
    </ul>
  </li>
  <li>max_delta_step [default=0]
    <ul>
      <li>In maximum delta step we allow each tree’s weight estimation to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help making the update step more conservative.</li>
      <li>Usually this parameter is not needed, but it might help in logistic regression when class is <strong>extremely imbalanced</strong>.</li>
      <li>This is generally not used but you can explore further if you wish.</li>
    </ul>
  </li>
  <li>subsample [default=1]
    <ul>
      <li>Same as the subsample of GBM. Denotes the fraction of observations to be randomly samples for each tree.</li>
      <li>Lower values make the algorithm more conservative and prevents overfitting but too small values might lead to under-fitting.</li>
      <li>Typical values: 0.5-1</li>
    </ul>
  </li>
  <li>colsample_bytree [default=1]
    <ul>
      <li>Similar to max_features in GBM. Denotes the fraction of columns to be randomly samples for each tree.</li>
      <li>Typical values: 0.5-1</li>
    </ul>
  </li>
  <li>colsample_bylevel [default=1]
    <ul>
      <li>Denotes the subsample ratio of columns for each split, in each level.</li>
      <li>I don’t use this often because subsample and colsample_bytree will do the job for you. but you can explore further if you feel so.</li>
    </ul>
  </li>
  <li>lambda [default=1]
    <ul>
      <li>L2 regularization term on weights (analogous to Ridge regression)</li>
      <li>This used to handle the regularization part of XGBoost. Though many data scientists don’t use it often, it should be explored to reduce <strong>overfitting</strong>.</li>
    </ul>
  </li>
  <li>alpha [default=0]
    <ul>
      <li>L1 regularization term on weight (analogous to Lasso regression)</li>
      <li>Can be used in case of very high dimensionality so that the algorithm runs faster when implemented</li>
    </ul>
  </li>
  <li>scale_pos_weight [default=1]
    <ul>
      <li>A value greater than 0 should be used in case of high class <strong>imbalance</strong> as it helps in faster convergence.</li>
    </ul>
  </li>
</ol>

<h3 id="learning-task-parameters">2.3 Learning Task Parameters</h3>

<ol>
  <li>objective [default=reg:linear]</li>
  <li>eval_metric [ default according to objective ]</li>
  <li>seed [default=0]</li>
</ol>

<p>XGBoost Parameters guide: <a href="http://xgboost.readthedocs.io/en/latest/parameter.html#general-parameters">official</a> <a href="https://github.com/dmlc/xgboost/blob/master/doc/parameter.md">github</a></p>

<h2 id="parameter-tuning">3. Parameter Tuning</h2>

<p>2 forms of XGBoost:</p>

<ol>
  <li><strong>xgb</strong> – this is the direct xgboost library. I will use a specific function “cv” from this library.</li>
  <li><strong>XGBClassifier</strong> – this is an sklearn wrapper for XGBoost. This allows us to use sklearn’s Grid Search with parallel processing in the same way we did for GBM.</li>
</ol>

<p>Lets define a function which will help us create XGBoost models and perform cross-validation.</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">modelfit</span><span class="p">(</span><span class="n">alg</span><span class="p">,</span> <span class="n">dtrain</span><span class="p">,</span> <span class="n">predictors</span><span class="p">,</span><span class="n">useTrainCV</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">cv_folds</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">early_stopping_rounds</span><span class="o">=</span><span class="mi">50</span><span class="p">):</span>
    
    <span class="k">if</span> <span class="n">useTrainCV</span><span class="p">:</span>
        <span class="n">xgb_param</span> <span class="o">=</span> <span class="n">alg</span><span class="o">.</span><span class="n">get_xgb_params</span><span class="p">()</span>
        <span class="n">xgtrain</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">DMatrix</span><span class="p">(</span><span class="n">dtrain</span><span class="p">[</span><span class="n">predictors</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">dtrain</span><span class="p">[</span><span class="n">target</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
        <span class="n">cvresult</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">cv</span><span class="p">(</span><span class="n">xgb_param</span><span class="p">,</span> <span class="n">xgtrain</span><span class="p">,</span> <span class="n">num_boost_round</span><span class="o">=</span><span class="n">alg</span><span class="o">.</span><span class="n">get_params</span><span class="p">()[</span><span class="s">'n_estimators'</span><span class="p">],</span> <span class="n">nfold</span><span class="o">=</span><span class="n">cv_folds</span><span class="p">,</span>
            <span class="n">metrics</span><span class="o">=</span><span class="s">'auc'</span><span class="p">,</span> <span class="n">early_stopping_rounds</span><span class="o">=</span><span class="n">early_stopping_rounds</span><span class="p">,</span> <span class="n">show_progress</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">alg</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="n">cvresult</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    
    <span class="c">#Fit the algorithm on the data</span>
    <span class="n">alg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">dtrain</span><span class="p">[</span><span class="n">predictors</span><span class="p">],</span> <span class="n">dtrain</span><span class="p">[</span><span class="s">'Disbursed'</span><span class="p">],</span><span class="n">eval_metric</span><span class="o">=</span><span class="s">'auc'</span><span class="p">)</span>
        
    <span class="c">#Predict training set:</span>
    <span class="n">dtrain_predictions</span> <span class="o">=</span> <span class="n">alg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">dtrain</span><span class="p">[</span><span class="n">predictors</span><span class="p">])</span>
    <span class="n">dtrain_predprob</span> <span class="o">=</span> <span class="n">alg</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">dtrain</span><span class="p">[</span><span class="n">predictors</span><span class="p">])[:,</span><span class="mi">1</span><span class="p">]</span>
        
    <span class="c">#Print model report:</span>
    <span class="k">print</span> <span class="s">"</span><span class="se">\n</span><span class="s">Model Report"</span>
    <span class="k">print</span> <span class="s">"Accuracy : </span><span class="si">%.4</span><span class="s">g"</span> <span class="o">%</span> <span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">dtrain</span><span class="p">[</span><span class="s">'Disbursed'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtrain_predictions</span><span class="p">)</span>
    <span class="k">print</span> <span class="s">"AUC Score (Train): </span><span class="si">%</span><span class="s">f"</span> <span class="o">%</span> <span class="n">metrics</span><span class="o">.</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">dtrain</span><span class="p">[</span><span class="s">'Disbursed'</span><span class="p">],</span> <span class="n">dtrain_predprob</span><span class="p">)</span>
                    
    <span class="n">feat_imp</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">alg</span><span class="o">.</span><span class="n">booster</span><span class="p">()</span><span class="o">.</span><span class="n">get_fscore</span><span class="p">())</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="n">feat_imp</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s">'bar'</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s">'Feature Importances'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Feature Importance Score'</span><span class="p">)</span>
</code></pre>
</div>

<h3 id="general-approach-for-parameter-tuning">General Approach for Parameter Tuning</h3>

<ol>
  <li>Choose a relatively <strong>high learning rate</strong>. Generally a learning rate of 0.1 works but somewhere between 0.05 to 0.3 should work for different problems. Determine the <strong>optimum number of trees</strong> for this learning rate. XGBoost has a very useful function called as “cv” which performs cross-validation at each boosting iteration and thus returns the optimum number of trees required.</li>
  <li>Tune <strong>tree-specific parameters</strong> ( max_depth, min_child_weight, gamma, subsample, colsample_bytree) for decided learning rate and number of trees. Note that we can choose different parameters to define a tree and I’ll take up an example here.</li>
  <li>Tune <strong>regularization parameters</strong> (lambda, alpha) for xgboost which can help reduce model complexity and enhance performance.</li>
  <li><strong>Lower the learning rate</strong> and decide the optimal parameters.</li>
</ol>

<h3 id="step-1-fix-learning-rate-and-number-of-estimators-for-tuning-tree-based-parameters">Step 1: Fix learning rate and number of estimators for tuning tree-based parameters</h3>

<p>In order to decide on boosting parameters, we need to set some initial values of other parameters.</p>

<ol>
  <li>max_depth = 5 : This should be between 3-10.</li>
  <li>min_child_weight = 1 : A smaller value is chosen because it is a highly imbalanced class problem and leaf nodes can have smaller size groups.</li>
  <li>gamma = 0 : A smaller value like 0.1-0.2 can also be chosen for starting.</li>
  <li>subsample, colsample_bytree = 0.8 : This is a commonly used used start value.</li>
  <li>scale_pos_weight = 1: Because of high class imbalance.</li>
</ol>

<p>Lets take the <strong>default learning rate</strong> of 0.1 here and check the optimum <strong>number of trees</strong> using cv function of xgboost.</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="n">predictors</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">train</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="n">x</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="n">target</span><span class="p">,</span> <span class="n">IDcol</span><span class="p">]]</span>
<span class="n">xgb1</span> <span class="o">=</span> <span class="n">XGBClassifier</span><span class="p">(</span>
    <span class="n">learning_rate</span> <span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">min_child_weight</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">gamma</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">subsample</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
    <span class="n">colsample_bytree</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
    <span class="n">objective</span><span class="o">=</span> <span class="s">'binary:logistic'</span><span class="p">,</span>
    <span class="n">nthread</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">scale_pos_weight</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">seed</span><span class="o">=</span><span class="mi">27</span><span class="p">)</span>
<span class="n">modelfit</span><span class="p">(</span><span class="n">xgb1</span><span class="p">,</span> <span class="n">train</span><span class="p">,</span> <span class="n">predictors</span><span class="p">)</span>
</code></pre>
</div>

<h3 id="step-2-tune-other-params">Step 2: Tune other params</h3>

<ol>
  <li>max_depth, min_child_weight</li>
  <li>gamma</li>
  <li>subsample, colsample_bytree</li>
  <li>reg_alpha</li>
  <li>reg_lambda</li>
</ol>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="n">param_test1</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">'max_depth'</span><span class="p">:</span><span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span>
    <span class="s">'min_child_weight'</span><span class="p">:</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="p">}</span>
<span class="n">gsearch1</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">estimator</span> <span class="o">=</span> <span class="n">XGBClassifier</span><span class="p">(</span> <span class="n">learning_rate</span> <span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">140</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">min_child_weight</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">subsample</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">colsample_bytree</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
    <span class="n">objective</span><span class="o">=</span> <span class="s">'binary:logistic'</span><span class="p">,</span> <span class="n">nthread</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">scale_pos_weight</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">27</span><span class="p">),</span> 
    <span class="n">param_grid</span> <span class="o">=</span> <span class="n">param_test1</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s">'roc_auc'</span><span class="p">,</span><span class="n">n_jobs</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span><span class="n">iid</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">gsearch1</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train</span><span class="p">[</span><span class="n">predictors</span><span class="p">],</span><span class="n">train</span><span class="p">[</span><span class="n">target</span><span class="p">])</span>
<span class="n">gsearch1</span><span class="o">.</span><span class="n">grid_scores_</span><span class="p">,</span> <span class="n">gsearch1</span><span class="o">.</span><span class="n">best_params_</span><span class="p">,</span> <span class="n">gsearch1</span><span class="o">.</span><span class="n">best_score_</span>
</code></pre>
</div>


  </div>

</article>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">ShengYg's Blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>ShengYg's Blog</li>
          <li><a href="mailto:shengy0726@gmail.com">shengy0726@gmail.com</a></li>
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/ShengYg">
<span class="icon icon--github"><?xml version="1.0" encoding="iso-8859-1"?>
<!-- Generator: Adobe Illustrator 16.0.0, SVG Export Plug-In . SVG Version: 6.00 Build 0)  -->
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" id="Capa_1" x="0px" y="0px" width="512px" height="512px" viewBox="0 0 438.549 438.549" style="enable-background:new 0 0 438.549 438.549;" xml:space="preserve">
<g>
	<path d="M409.132,114.573c-19.608-33.596-46.205-60.194-79.798-79.8C295.736,15.166,259.057,5.365,219.271,5.365   c-39.781,0-76.472,9.804-110.063,29.408c-33.596,19.605-60.192,46.204-79.8,79.8C9.803,148.168,0,184.854,0,224.63   c0,47.78,13.94,90.745,41.827,128.906c27.884,38.164,63.906,64.572,108.063,79.227c5.14,0.954,8.945,0.283,11.419-1.996   c2.475-2.282,3.711-5.14,3.711-8.562c0-0.571-0.049-5.708-0.144-15.417c-0.098-9.709-0.144-18.179-0.144-25.406l-6.567,1.136   c-4.187,0.767-9.469,1.092-15.846,1c-6.374-0.089-12.991-0.757-19.842-1.999c-6.854-1.231-13.229-4.086-19.13-8.559   c-5.898-4.473-10.085-10.328-12.56-17.556l-2.855-6.57c-1.903-4.374-4.899-9.233-8.992-14.559   c-4.093-5.331-8.232-8.945-12.419-10.848l-1.999-1.431c-1.332-0.951-2.568-2.098-3.711-3.429c-1.142-1.331-1.997-2.663-2.568-3.997   c-0.572-1.335-0.098-2.43,1.427-3.289c1.525-0.859,4.281-1.276,8.28-1.276l5.708,0.853c3.807,0.763,8.516,3.042,14.133,6.851   c5.614,3.806,10.229,8.754,13.846,14.842c4.38,7.806,9.657,13.754,15.846,17.847c6.184,4.093,12.419,6.136,18.699,6.136   c6.28,0,11.704-0.476,16.274-1.423c4.565-0.952,8.848-2.383,12.847-4.285c1.713-12.758,6.377-22.559,13.988-29.41   c-10.848-1.14-20.601-2.857-29.264-5.14c-8.658-2.286-17.605-5.996-26.835-11.14c-9.235-5.137-16.896-11.516-22.985-19.126   c-6.09-7.614-11.088-17.61-14.987-29.979c-3.901-12.374-5.852-26.648-5.852-42.826c0-23.035,7.52-42.637,22.557-58.817   c-7.044-17.318-6.379-36.732,1.997-58.24c5.52-1.715,13.706-0.428,24.554,3.853c10.85,4.283,18.794,7.952,23.84,10.994   c5.046,3.041,9.089,5.618,12.135,7.708c17.705-4.947,35.976-7.421,54.818-7.421s37.117,2.474,54.823,7.421l10.849-6.849   c7.419-4.57,16.18-8.758,26.262-12.565c10.088-3.805,17.802-4.853,23.134-3.138c8.562,21.509,9.325,40.922,2.279,58.24   c15.036,16.18,22.559,35.787,22.559,58.817c0,16.178-1.958,30.497-5.853,42.966c-3.9,12.471-8.941,22.457-15.125,29.979   c-6.191,7.521-13.901,13.85-23.131,18.986c-9.232,5.14-18.182,8.85-26.84,11.136c-8.662,2.286-18.415,4.004-29.263,5.146   c9.894,8.562,14.842,22.077,14.842,40.539v60.237c0,3.422,1.19,6.279,3.572,8.562c2.379,2.279,6.136,2.95,11.276,1.995   c44.163-14.653,80.185-41.062,108.068-79.226c27.88-38.161,41.825-81.126,41.825-128.906   C438.536,184.851,428.728,148.168,409.132,114.573z" fill="#808080"/>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
</svg>
</span>
<span class="username">ShengYg</span>
</a>

          </li>
          

          

          

          

        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>Step after step the ladder is ascended.
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
