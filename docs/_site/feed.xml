<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ShengYg&#39;s Blog</title>
    <description>Step after step the ladder is ascended.
</description>
    <link>https://shengyg.github.io/repository/docs/</link>
    <atom:link href="https://shengyg.github.io/repository/docs/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Mon, 26 Jun 2017 17:33:03 +0800</pubDate>
    <lastBuildDate>Mon, 26 Jun 2017 17:33:03 +0800</lastBuildDate>
    <generator>Jekyll v3.0.1</generator>
    
      <item>
        <title>Welcome to Jekyll!</title>
        <description>&lt;p&gt;You’ll find this post in your &lt;code class=&quot;highlighter-rouge&quot;&gt;_posts&lt;/code&gt; directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run &lt;code class=&quot;highlighter-rouge&quot;&gt;jekyll serve&lt;/code&gt;, which launches a web server and auto-regenerates your site when a file is updated.&lt;/p&gt;

&lt;p&gt;To add new posts, simply add a file in the &lt;code class=&quot;highlighter-rouge&quot;&gt;_posts&lt;/code&gt; directory that follows the convention &lt;code class=&quot;highlighter-rouge&quot;&gt;YYYY-MM-DD-name-of-post.ext&lt;/code&gt; and includes the necessary front matter. Take a look at the source for this post to get an idea about how it works.&lt;/p&gt;

&lt;p&gt;Jekyll also offers powerful support for code snippets:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Hi, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;#{&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;Tom&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#=&amp;gt; prints &#39;Hi, Tom&#39; to STDOUT.&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Check out the &lt;a href=&quot;http://jekyllrb.com/docs/home&quot;&gt;Jekyll docs&lt;/a&gt; for more info on how to get the most out of Jekyll. File all bugs/feature requests at &lt;a href=&quot;https://github.com/jekyll/jekyll&quot;&gt;Jekyll’s GitHub repo&lt;/a&gt;. If you have questions, you can ask them on &lt;a href=&quot;https://talk.jekyllrb.com/&quot;&gt;Jekyll Talk&lt;/a&gt;.&lt;/p&gt;

</description>
        <pubDate>Mon, 19 Jun 2017 21:56:41 +0800</pubDate>
        <link>https://shengyg.github.io/repository/docs/jekyll/update/2017/06/19/welcome-to-jekyll.html</link>
        <guid isPermaLink="true">https://shengyg.github.io/repository/docs/jekyll/update/2017/06/19/welcome-to-jekyll.html</guid>
        
        
        <category>jekyll</category>
        
        <category>update</category>
        
      </item>
    
      <item>
        <title>example</title>
        <description>&lt;p&gt;Text can be &lt;strong&gt;bold&lt;/strong&gt;, &lt;em&gt;italic&lt;/em&gt;, or &lt;del&gt;strikethrough&lt;/del&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/about&quot;&gt;Link to another page: About&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;inline &lt;code class=&quot;highlighter-rouge&quot;&gt;code&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;header-1&quot;&gt;Header 1&lt;/h2&gt;

&lt;h3 id=&quot;header-2&quot;&gt;Header 2&lt;/h3&gt;

&lt;h4 id=&quot;header-3&quot;&gt;Header 3&lt;/h4&gt;

&lt;h5 id=&quot;header-4&quot;&gt;Header 4&lt;/h5&gt;

&lt;h6 id=&quot;header-5&quot;&gt;Header 5&lt;/h6&gt;

&lt;h6 id=&quot;header-6&quot;&gt;# Header 6&lt;/h6&gt;

&lt;blockquote&gt;
  &lt;p&gt;This is a blockquote following a header.&lt;/p&gt;

  &lt;p&gt;When something is important enough, you do it even if the odds are not in your favor.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// Javascript code with syntax highlighting.&lt;/span&gt;
&lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;fun&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;lang&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;dateformat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;i18n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;require&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;./lang/&#39;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;head1&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;head two&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;three&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;ok&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;good swedish fish&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;nice&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;out of stock&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;good and plenty&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;nice&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;ok&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;good &lt;code class=&quot;highlighter-rouge&quot;&gt;oreos&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;hmm&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;ok&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;good &lt;code class=&quot;highlighter-rouge&quot;&gt;zoute&lt;/code&gt; drop&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;yumm&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;theres-a-horizontal-rule-below-this&quot;&gt;There’s a horizontal rule below this.&lt;/h3&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;here-is-an-unordered-list&quot;&gt;Here is an unordered list:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Item foo&lt;/li&gt;
  &lt;li&gt;Item bar&lt;/li&gt;
  &lt;li&gt;Item baz&lt;/li&gt;
  &lt;li&gt;Item zip&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;and-an-ordered-list&quot;&gt;And an ordered list:&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Item one&lt;/li&gt;
  &lt;li&gt;Item two&lt;/li&gt;
  &lt;li&gt;Item three&lt;/li&gt;
  &lt;li&gt;Item four&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;and-a-nested-list&quot;&gt;And a nested list:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;level 1 item
    &lt;ul&gt;
      &lt;li&gt;level 2 item&lt;/li&gt;
      &lt;li&gt;level 2 item
        &lt;ul&gt;
          &lt;li&gt;level 3 item&lt;/li&gt;
          &lt;li&gt;level 3 item&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;level 1 item
    &lt;ul&gt;
      &lt;li&gt;level 2 item&lt;/li&gt;
      &lt;li&gt;level 2 item&lt;/li&gt;
      &lt;li&gt;level 2 item&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;level 1 item
    &lt;ul&gt;
      &lt;li&gt;level 2 item&lt;/li&gt;
      &lt;li&gt;level 2 item&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;level 1 item&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;small-image&quot;&gt;Small image&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://assets-cdn.github.com/images/icons/emoji/octocat.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;large-image&quot;&gt;Large image&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://guides.github.com/activities/hello-world/branching.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Long, single-line code blocks should not wrap. They should horizontally scroll if they are too long. This line should be long enough to demonstrate this.
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;
Long, single-line code blocks should not wrap. They should horizontally scroll if they are too long. This line should be long enough to demonstrate this.
&lt;/code&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 19 Jun 2017 21:56:41 +0800</pubDate>
        <link>https://shengyg.github.io/repository/docs/jekyll/update/2017/06/19/example-content.html</link>
        <guid isPermaLink="true">https://shengyg.github.io/repository/docs/jekyll/update/2017/06/19/example-content.html</guid>
        
        
        <category>jekyll</category>
        
        <category>update</category>
        
      </item>
    
      <item>
        <title>Complete Guide to Parameter Tuning in Xgboost</title>
        <description>&lt;h2 id=&quot;table-of-contents&quot;&gt;Table of Contents&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;The XGBoost Advantage&lt;/li&gt;
  &lt;li&gt;Understanding XGBoost Parameters&lt;/li&gt;
  &lt;li&gt;Tuning Parameters&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;the-xgboost-advantage&quot;&gt;1. The XGBoost Advantage&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Regularization:
    &lt;ul&gt;
      &lt;li&gt;Standard GBM implementation has &lt;strong&gt;no regularization&lt;/strong&gt; like XGBoost, therefore it also helps to reduce overfitting.&lt;/li&gt;
      &lt;li&gt;In fact, XGBoost is also known as &lt;strong&gt;‘regularized boosting’&lt;/strong&gt; technique.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Parallel Processing:
    &lt;ul&gt;
      &lt;li&gt;XGBoost implements parallel processing and is blazingly faster as compared to GBM.&lt;/li&gt;
      &lt;li&gt;Check &lt;a href=&quot;http://zhanpengfang.github.io/418home.html&quot;&gt;this link&lt;/a&gt; out to explore further.&lt;/li&gt;
      &lt;li&gt;XGBoost also supports implementation on Hadoop.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;High Flexibility
    &lt;ul&gt;
      &lt;li&gt;XGBoost allow users to define &lt;strong&gt;custom optimization objectives and evaluation criteria&lt;/strong&gt;.&lt;/li&gt;
      &lt;li&gt;This adds a whole new dimension to the model and there is no limit to what we can do.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Handling Missing Values
    &lt;ul&gt;
      &lt;li&gt;XGBoost has an in-built routine to handle missing values.&lt;/li&gt;
      &lt;li&gt;User is required to supply a different value than other observations and pass that as a parameter. XGBoost tries different things as it encounters a missing value on each node and learns which path to take for missing values in future.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Tree Pruning:
    &lt;ul&gt;
      &lt;li&gt;A GBM would stop splitting a node when it encounters a negative loss in the split. Thus it is more of a &lt;strong&gt;greedy algorithm&lt;/strong&gt;.&lt;/li&gt;
      &lt;li&gt;XGBoost on the other hand make &lt;strong&gt;splits upto the max_depth&lt;/strong&gt; specified and then start &lt;strong&gt;pruning&lt;/strong&gt; the tree backwards and remove splits beyond which there is no positive gain.&lt;/li&gt;
      &lt;li&gt;Another advantage is that sometimes a split of negative loss say -2 may be followed by a split of positive loss +10. GBM would stop as it encounters -2. But XGBoost will go deeper and it will see a combined effect of +8 of the split and keep both.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Built-in Cross-Validation
    &lt;ul&gt;
      &lt;li&gt;XGBoost allows user to run a cross-validation at each iteration of the boosting process and thus it is easy to get the exact optimum number of boosting iterations in a single run.&lt;/li&gt;
      &lt;li&gt;This is unlike GBM where we have to run a grid-search and only a limited values can be tested.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Continue on Existing Model
    &lt;ul&gt;
      &lt;li&gt;User can start training an XGBoost model from its last iteration of previous run. This can be of significant advantage in certain specific applications.&lt;/li&gt;
      &lt;li&gt;GBM implementation of sklearn also has this feature so they are even on this point.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;xgboost-parameters&quot;&gt;2. XGBoost Parameters&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;General Parameters&lt;/strong&gt;: Guide the overall functioning&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Booster Parameters&lt;/strong&gt;: Guide the individual booster (tree/regression) at each step&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Learning Task Parameters&lt;/strong&gt;: Guide the optimization performed&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;general-parameters&quot;&gt;2.1 General Parameters&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;booster [default=gbtree]&lt;/li&gt;
  &lt;li&gt;silent [default=0]&lt;/li&gt;
  &lt;li&gt;nthread [default to maximum number of threads available if not set]&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;booster-parameters&quot;&gt;2.2 Booster Parameters&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;eta [default=0.3]
    &lt;ul&gt;
      &lt;li&gt;Typical final values to be used: 0.01-0.2&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;min_child_weight [default=1]
    &lt;ul&gt;
      &lt;li&gt;Defines the minimum sum of weights of all observations required in a child.&lt;/li&gt;
      &lt;li&gt;This is similar to &lt;strong&gt;min_child_leaf&lt;/strong&gt; in GBM but not exactly. This refers to min “sum of weights” of observations while GBM has min “number of observations”.&lt;/li&gt;
      &lt;li&gt;Used to control &lt;strong&gt;over-fitting&lt;/strong&gt;. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.&lt;/li&gt;
      &lt;li&gt;Too high values can lead to under-fitting hence, it should be tuned using CV.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;max_depth [default=6]
    &lt;ul&gt;
      &lt;li&gt;Used to control &lt;strong&gt;over-fitting&lt;/strong&gt; as higher depth will allow model to learn relations very specific to a particular sample.&lt;/li&gt;
      &lt;li&gt;Typical values: 3-10&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;max_leaf_nodes
    &lt;ul&gt;
      &lt;li&gt;The maximum number of terminal nodes or leaves in a tree.&lt;/li&gt;
      &lt;li&gt;Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves.&lt;/li&gt;
      &lt;li&gt;If this is defined, GBM will ignore max_depth.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;gamma [default=0]
    &lt;ul&gt;
      &lt;li&gt;A node is split only when the resulting split gives a positive reduction in the loss function. Gamma specifies the minimum loss reduction required to make a split.&lt;/li&gt;
      &lt;li&gt;Makes the algorithm conservative. The values can vary &lt;strong&gt;depending on the loss function&lt;/strong&gt; and should be tuned.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;max_delta_step [default=0]
    &lt;ul&gt;
      &lt;li&gt;In maximum delta step we allow each tree’s weight estimation to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help making the update step more conservative.&lt;/li&gt;
      &lt;li&gt;Usually this parameter is not needed, but it might help in logistic regression when class is &lt;strong&gt;extremely imbalanced&lt;/strong&gt;.&lt;/li&gt;
      &lt;li&gt;This is generally not used but you can explore further if you wish.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;subsample [default=1]
    &lt;ul&gt;
      &lt;li&gt;Same as the subsample of GBM. Denotes the fraction of observations to be randomly samples for each tree.&lt;/li&gt;
      &lt;li&gt;Lower values make the algorithm more conservative and prevents overfitting but too small values might lead to under-fitting.&lt;/li&gt;
      &lt;li&gt;Typical values: 0.5-1&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;colsample_bytree [default=1]
    &lt;ul&gt;
      &lt;li&gt;Similar to max_features in GBM. Denotes the fraction of columns to be randomly samples for each tree.&lt;/li&gt;
      &lt;li&gt;Typical values: 0.5-1&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;colsample_bylevel [default=1]
    &lt;ul&gt;
      &lt;li&gt;Denotes the subsample ratio of columns for each split, in each level.&lt;/li&gt;
      &lt;li&gt;I don’t use this often because subsample and colsample_bytree will do the job for you. but you can explore further if you feel so.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;lambda [default=1]
    &lt;ul&gt;
      &lt;li&gt;L2 regularization term on weights (analogous to Ridge regression)&lt;/li&gt;
      &lt;li&gt;This used to handle the regularization part of XGBoost. Though many data scientists don’t use it often, it should be explored to reduce &lt;strong&gt;overfitting&lt;/strong&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;alpha [default=0]
    &lt;ul&gt;
      &lt;li&gt;L1 regularization term on weight (analogous to Lasso regression)&lt;/li&gt;
      &lt;li&gt;Can be used in case of very high dimensionality so that the algorithm runs faster when implemented&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;scale_pos_weight [default=1]
    &lt;ul&gt;
      &lt;li&gt;A value greater than 0 should be used in case of high class &lt;strong&gt;imbalance&lt;/strong&gt; as it helps in faster convergence.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;learning-task-parameters&quot;&gt;2.3 Learning Task Parameters&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;objective [default=reg:linear]&lt;/li&gt;
  &lt;li&gt;eval_metric [ default according to objective ]&lt;/li&gt;
  &lt;li&gt;seed [default=0]&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;XGBoost Parameters guide: &lt;a href=&quot;http://xgboost.readthedocs.io/en/latest/parameter.html#general-parameters&quot;&gt;official&lt;/a&gt; &lt;a href=&quot;https://github.com/dmlc/xgboost/blob/master/doc/parameter.md&quot;&gt;github&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;parameter-tuning&quot;&gt;3. Parameter Tuning&lt;/h2&gt;

&lt;p&gt;2 forms of XGBoost:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;xgb&lt;/strong&gt; – this is the direct xgboost library. I will use a specific function “cv” from this library.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;XGBClassifier&lt;/strong&gt; – this is an sklearn wrapper for XGBoost. This allows us to use sklearn’s Grid Search with parallel processing in the same way we did for GBM.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Lets define a function which will help us create XGBoost models and perform cross-validation.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;modelfit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtrain&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predictors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;useTrainCV&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv_folds&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;early_stopping_rounds&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;useTrainCV&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;xgb_param&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_xgb_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;xgtrain&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xgb&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DMatrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtrain&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predictors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtrain&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;cvresult&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xgb&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb_param&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xgtrain&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_boost_round&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;n_estimators&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nfold&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv_folds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;auc&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;early_stopping_rounds&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;early_stopping_rounds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;show_progress&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;alg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_estimators&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cvresult&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    
    &lt;span class=&quot;c&quot;&gt;#Fit the algorithm on the data&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;alg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtrain&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predictors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtrain&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;Disbursed&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eval_metric&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;auc&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
    &lt;span class=&quot;c&quot;&gt;#Predict training set:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dtrain_predictions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtrain&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predictors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dtrain_predprob&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict_proba&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtrain&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predictors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])[:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        
    &lt;span class=&quot;c&quot;&gt;#Print model report:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Model Report&quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Accuracy : &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%.4&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;g&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;accuracy_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtrain&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;Disbursed&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtrain_predictions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;AUC Score (Train): &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;roc_auc_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtrain&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;Disbursed&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtrain_predprob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                    
    &lt;span class=&quot;n&quot;&gt;feat_imp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Series&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;booster&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_fscore&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sort_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ascending&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;feat_imp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;bar&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;Feature Importances&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;Feature Importance Score&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;general-approach-for-parameter-tuning&quot;&gt;General Approach for Parameter Tuning&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Choose a relatively &lt;strong&gt;high learning rate&lt;/strong&gt;. Generally a learning rate of 0.1 works but somewhere between 0.05 to 0.3 should work for different problems. Determine the &lt;strong&gt;optimum number of trees&lt;/strong&gt; for this learning rate. XGBoost has a very useful function called as “cv” which performs cross-validation at each boosting iteration and thus returns the optimum number of trees required.&lt;/li&gt;
  &lt;li&gt;Tune &lt;strong&gt;tree-specific parameters&lt;/strong&gt; ( max_depth, min_child_weight, gamma, subsample, colsample_bytree) for decided learning rate and number of trees. Note that we can choose different parameters to define a tree and I’ll take up an example here.&lt;/li&gt;
  &lt;li&gt;Tune &lt;strong&gt;regularization parameters&lt;/strong&gt; (lambda, alpha) for xgboost which can help reduce model complexity and enhance performance.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Lower the learning rate&lt;/strong&gt; and decide the optimal parameters.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;step-1-fix-learning-rate-and-number-of-estimators-for-tuning-tree-based-parameters&quot;&gt;Step 1: Fix learning rate and number of estimators for tuning tree-based parameters&lt;/h3&gt;

&lt;p&gt;In order to decide on boosting parameters, we need to set some initial values of other parameters.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;max_depth = 5 : This should be between 3-10.&lt;/li&gt;
  &lt;li&gt;min_child_weight = 1 : A smaller value is chosen because it is a highly imbalanced class problem and leaf nodes can have smaller size groups.&lt;/li&gt;
  &lt;li&gt;gamma = 0 : A smaller value like 0.1-0.2 can also be chosen for starting.&lt;/li&gt;
  &lt;li&gt;subsample, colsample_bytree = 0.8 : This is a commonly used used start value.&lt;/li&gt;
  &lt;li&gt;scale_pos_weight = 1: Because of high class imbalance.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Lets take the &lt;strong&gt;default learning rate&lt;/strong&gt; of 0.1 here and check the optimum &lt;strong&gt;number of trees&lt;/strong&gt; using cv function of xgboost.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;predictors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;IDcol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;xgb1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;XGBClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n_estimators&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;max_depth&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;min_child_weight&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;gamma&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;subsample&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;colsample_bytree&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;objective&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;binary:logistic&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;nthread&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;scale_pos_weight&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;27&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;modelfit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predictors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;step-2-tune-other-params&quot;&gt;Step 2: Tune other params&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;max_depth, min_child_weight&lt;/li&gt;
  &lt;li&gt;gamma&lt;/li&gt;
  &lt;li&gt;subsample, colsample_bytree&lt;/li&gt;
  &lt;li&gt;reg_alpha&lt;/li&gt;
  &lt;li&gt;reg_lambda&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;param_test1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&#39;max_depth&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&#39;min_child_weight&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;gsearch1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GridSearchCV&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;estimator&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;XGBClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_estimators&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;140&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max_depth&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;min_child_weight&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gamma&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;subsample&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;colsample_bytree&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;objective&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;binary:logistic&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nthread&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scale_pos_weight&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;27&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;param_grid&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;param_test1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scoring&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;roc_auc&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_jobs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iid&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;gsearch1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predictors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;gsearch1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grid_scores_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gsearch1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;best_params_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gsearch1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;best_score_&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

</description>
        <pubDate>Mon, 19 Jun 2017 12:00:00 +0800</pubDate>
        <link>https://shengyg.github.io/repository/docs/jekyll/update/2017/06/19/Complete-Guide-to-Parameter-Tuning-xgboost.html</link>
        <guid isPermaLink="true">https://shengyg.github.io/repository/docs/jekyll/update/2017/06/19/Complete-Guide-to-Parameter-Tuning-xgboost.html</guid>
        
        
        <category>jekyll</category>
        
        <category>update</category>
        
      </item>
    
      <item>
        <title>Complete Guide to Parameter Tuning in GBM</title>
        <description>&lt;h2 id=&quot;table-of-contents&quot;&gt;Table of Contents&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;How Boosting Works&lt;/li&gt;
  &lt;li&gt;Understanding GBM Parameters&lt;/li&gt;
  &lt;li&gt;Tuning Parameters&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;how-boosting-works&quot;&gt;1. How Boosting Works&lt;/h2&gt;
&lt;p&gt;Boosting is a sequential technique which works on the principle of &lt;strong&gt;ensemble&lt;/strong&gt;. It combines a set of &lt;strong&gt;weak learners&lt;/strong&gt; and delivers improved prediction accuracy. At any instant t, the model outcomes are weighed based on the outcomes of previous instant t-1. The outcomes predicted correctly are given a lower weight and the ones miss-classified are weighted higher.&lt;/p&gt;

&lt;h2 id=&quot;gbm-parameters&quot;&gt;2. GBM Parameters&lt;/h2&gt;
&lt;p&gt;The overall parameters can be divided into 3 categories:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Tree-Specific Parameters&lt;/strong&gt;: These affect each individual tree in the model.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Boosting Parameters&lt;/strong&gt;: These affect the boosting operation in the model.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Miscellaneous Parameters&lt;/strong&gt;: Other parameters for overall functioning.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;tree-specific-parameters&quot;&gt;2.1 Tree-Specific Parameters&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://shengyg.github.io/repository/docs/assets/0_00.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;min_samples_split&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Defines the minimum number of samples (or observations) which are required in a node to be considered for splitting.&lt;/li&gt;
      &lt;li&gt;Used to &lt;strong&gt;control over-fitting&lt;/strong&gt;. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.&lt;/li&gt;
      &lt;li&gt;Too high values can lead to under-fitting hence, it should be tuned using CV.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;min_samples_leaf&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Defines the minimum samples (or observations) required in a terminal node or leaf.&lt;/li&gt;
      &lt;li&gt;Used to control over-fitting similar to min_samples_split.&lt;/li&gt;
      &lt;li&gt;Generally lower values should be chosen for &lt;strong&gt;imbalanced&lt;/strong&gt; class problems because the regions in which the minority class will be in majority will be very small.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;min_weight_fraction_leaf&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Similar to min_samples_leaf but defined as a fraction of the total number of observations instead of an integer.&lt;/li&gt;
      &lt;li&gt;Only one of #2 and #3 should be defined.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;max_depth&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;The maximum depth of a tree.&lt;/li&gt;
      &lt;li&gt;Used to &lt;strong&gt;control over-fitting&lt;/strong&gt; as higher depth will allow model to learn relations very specific to a particular sample.&lt;/li&gt;
      &lt;li&gt;Should be tuned using CV.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;max_leaf_nodes&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;The maximum number of terminal nodes or leaves in a tree.&lt;/li&gt;
      &lt;li&gt;Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves.&lt;/li&gt;
      &lt;li&gt;If this is defined, GBM will ignore max_depth.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;max_features&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;The number of features to consider while searching for a best split. These will be randomly selected.&lt;/li&gt;
      &lt;li&gt;As a thumb-rule, square root of the total number of features works great but we should check upto &lt;strong&gt;30-40%&lt;/strong&gt; of the total number of features.&lt;/li&gt;
      &lt;li&gt;Higher values can lead to over-fitting but depends on case to case.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;boosting-parameters&quot;&gt;2.2 Boosting Parameters&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;learning_rate&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;This determines the impact of each tree on the final outcome. GBM works by starting with an initial estimate which is updated using the output of each tree. The learning parameter controls the magnitude of this change in the estimates.&lt;/li&gt;
      &lt;li&gt;Lower values are generally preferred as they make the model robust to the specific characteristics of tree and thus allowing it to generalize well.&lt;/li&gt;
      &lt;li&gt;Lower values would require &lt;strong&gt;higher number of trees&lt;/strong&gt; to model all the relations and will be computationally expensive.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;n_estimators&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;The number of sequential trees to be modeled.&lt;/li&gt;
      &lt;li&gt;Though GBM is fairly robust at higher number of trees but it can still &lt;strong&gt;overfit at a point&lt;/strong&gt;. Hence, this should be tuned using CV for a particular learning rate.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;subsample&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;The fraction of observations to be selected for each tree. Selection is done by random sampling.&lt;/li&gt;
      &lt;li&gt;Values slightly less than 1 make the model robust by reducing the variance.&lt;/li&gt;
      &lt;li&gt;Typical values &lt;strong&gt;~0.8&lt;/strong&gt; generally work fine but can be fine-tuned further.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;miscellaneous-parameters&quot;&gt;2.3 Miscellaneous Parameters&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;loss&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;It refers to the loss function to be minimized in each split.&lt;/li&gt;
      &lt;li&gt;It can have various values for classification and regression case. Generally the default values work fine. Other values should be chosen only if you understand their impact on the model.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;init&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;This affects initialization of the output.&lt;/li&gt;
      &lt;li&gt;This can be used if we have made another model whose outcome is to be used as the initial estimates for GBM.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;random_state&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;The random number seed so that same random numbers are generated every time.&lt;/li&gt;
      &lt;li&gt;This is important for parameter tuning. If we don’t fix the random number, then we’ll have different outcomes for subsequent runs on the same parameters and it becomes difficult to compare models.&lt;/li&gt;
      &lt;li&gt;It can potentially result in overfitting to a particular random sample selected. We can try running models for different random samples, which is computationally expensive and generally not used.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;verbose&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;The type of output to be printed when the model fits. The different values can be:&lt;/li&gt;
      &lt;li&gt;0: no output generated (default)&lt;/li&gt;
      &lt;li&gt;1: output generated for trees in certain intervals&lt;/li&gt;
      &lt;li&gt;&amp;gt;1: output generated for all trees&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;warm_start&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;This parameter has an interesting application and can help a lot if used judicially.&lt;/li&gt;
      &lt;li&gt;Using this, we can fit additional trees on previous fits of a model. It can save a lot of time and you should explore this option for advanced applications&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;presort&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Select whether to presort data for faster splits.&lt;/li&gt;
      &lt;li&gt;It makes the selection automatically by default but it can be changed if needed.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;tuning-parameters&quot;&gt;3. Tuning Parameters&lt;/h2&gt;

&lt;h3 id=&quot;general-approach-for-parameter-tuning&quot;&gt;3.1 General Approach for Parameter Tuning&lt;/h3&gt;

&lt;p&gt;As discussed earlier, there are two types of parameter to be tuned here – tree based and boosting parameters. There are no optimum values for learning rate as low values always work better, given that we train on sufficient number of trees.&lt;/p&gt;

&lt;p&gt;Though, GBM is robust enough to not overfit with increasing trees, but a high number for pa particular learning rate can lead to overfitting. But as we reduce the learning rate and increase trees, the computation becomes expensive and would take a long time to run on standard personal computers.&lt;/p&gt;

&lt;p&gt;Keeping all this in mind, we can take the following approach:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Choose a relatively &lt;strong&gt;high learning rate&lt;/strong&gt;. Generally the default value of 0.1 works but somewhere between 0.05 to 0.2 should work for different problems&lt;/li&gt;
  &lt;li&gt;Determine the &lt;strong&gt;optimum number of trees for this learning rate&lt;/strong&gt;. This should range around 40-70. Remember to choose a value on which your system can work fairly fast. This is because it will be used for testing various scenarios and determining the tree parameters.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Tune tree-specific parameters&lt;/strong&gt; for decided learning rate and number of trees. Note that we can choose different parameters to define a tree and I’ll take up an example here.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Lower the learning rate&lt;/strong&gt; and increase the estimators proportionally to get more robust models.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;fix-learning-rate-and-number-of-estimators-for-tuning-tree-based-parameters&quot;&gt;3.2 Fix learning rate and number of estimators for tuning tree-based parameters&lt;/h3&gt;

&lt;p&gt;we need to set some initial values of other parameters&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;min_samples_split = 500&lt;/strong&gt; : This should be ~0.5-1% of total values. Since this is imbalanced class problem, we’ll take a small value from the range.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;min_samples_leaf = 50&lt;/strong&gt; : Can be selected based on intuition. This is just used for preventing overfitting and again a small value because of imbalanced classes.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;max_depth = 8&lt;/strong&gt; : Should be chosen (5-8) based on the number of observations and predictors. This has 87K rows and 49 columns so lets take 8 here.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;max_features = ‘sqrt’&lt;/strong&gt; : Its a general thumb-rule to start with square root.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;subsample = 0.8&lt;/strong&gt; : This is a commonly used used start value.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;we can do a grid search and test out values from 20 to 80 in steps of 10.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#Choose all predictors except target &amp;amp; IDcols&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;predictors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;IDcol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;param_test1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;n_estimators&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;81&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;gsearch1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GridSearchCV&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;estimator&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GradientBoostingClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;min_samples_split&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;min_samples_leaf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max_depth&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max_features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;sqrt&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subsample&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;param_grid&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;param_test1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scoring&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;roc_auc&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_jobs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iid&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;gsearch1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predictors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The output can be checked using following command:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;gsearch1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grid_scores_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gsearch1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;best_params_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gsearch1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;best_score_&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;https://shengyg.github.io/repository/docs/assets/0_01.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;tuning-tree-specific-parameters&quot;&gt;3.3 Tuning tree-specific parameters&lt;/h3&gt;

&lt;p&gt;Now lets move onto tuning the tree parameters&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;max_depth and num_samples_split&lt;/li&gt;
  &lt;li&gt;min_samples_leaf&lt;/li&gt;
  &lt;li&gt;max_features&lt;/li&gt;
  &lt;li&gt;subsample&lt;/li&gt;
  &lt;li&gt;n_estimators&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The &lt;strong&gt;order of tuning&lt;/strong&gt; variables should be decided carefully. You should take the variables with a higher impact on outcome first. For instance, max_depth and min_samples_split have a significant impact and we’re tuning those first.&lt;/p&gt;

</description>
        <pubDate>Mon, 19 Jun 2017 10:00:00 +0800</pubDate>
        <link>https://shengyg.github.io/repository/docs/jekyll/update/2017/06/19/Complete-Guide-to-Parameter-Tuning-GBM.html</link>
        <guid isPermaLink="true">https://shengyg.github.io/repository/docs/jekyll/update/2017/06/19/Complete-Guide-to-Parameter-Tuning-GBM.html</guid>
        
        
        <category>jekyll</category>
        
        <category>update</category>
        
      </item>
    
  </channel>
</rss>
